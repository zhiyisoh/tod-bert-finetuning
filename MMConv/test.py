org = {'do_train': True,  
 'epoch': 300, 
 'patience': 10, 
 'earlystop': 'joint_acc', 
 'my_model': 'BeliefTracker', 
 'dropout': 0.2, 
 'learning_rate': 5e-05, 
 'batch_size': 6, 
 'eval_batch_size': 6, 
 'hdd_size': 400, 
 'emb_size': 400, 
 'grad_clip': 1, 
 'teacher_forcing_ratio': 0.5, 
 'load_embedding': False, 
 'fix_embedding': False, 
 'n_gpu': 1, 
 'eval_by_step': 4000, 
 'fix_encoder': False, 
 'model_type': 'bert', 
 'model_name_or_path': 'bert-base-uncased', 
 'usr_token': '[USR]', 
 'sys_token': '[SYS]', 
 'warmup_proportion': 0.1, 
 'local_rank': -1, 
 'gradient_accumulation_steps': 1, 
 'weight_decay': 0.0, 
 'adam_epsilon': 1e-08, 
 'warmup_steps': 0, 
 'fp16': False, 
 'fp16_opt_level': 'O1', 
 'output_mode': 'classification', 
 'max_steps': -1, 
 'rand_seed': 0, 
 'fix_rand_seed': False, 
 'nb_runs': 1, 
 'nb_evals': 1, 
 'max_seq_length': 512, 
 'input_name': 'context', 
 'data_path': '/export/home/dialog_datasets', 
 'task': 'dst', 
 'task_name': 'dst', 
 'example_type': 'turn', 
 'dataset': '["multiwoz"]', 
 'load_path': None, 
 'add_name': '', 
 'max_line': None, 
 'output_dir': 'save/BERT/DST/MWOZ', 
 'overwrite': False, 'cache_dir': None, 
 'logging_steps': 500, 
 'save_steps': 1000, 
 'save_total_limit': 1, 
 'train_data_ratio': 1.0, 
 'domain_act': False, 
 'only_last_turn': False, 
 'error_analysis': False, 
 'not_save_model': False, 
 'nb_shots': -1, 
 'do_embeddings': False,
 'create_own_vocab': False, 
 'unk_mask': True, 
 'parallel_decode': True, 
 'self_supervised': 'generative', 
 'oracle_domain': False, 
 'more_linear_mapping': False, 
 'gate_supervision_for_dst': False, 
 'sum_token_emb_for_value': False, 
 'nb_neg_sample_rs': 0, 
 'sample_negative_by_kmeans': False, 
 'nb_kmeans': 1000, 
 'bidirect': False, 
 'rnn_type': 'gru',
 'num_rnn_layers': 1, 
 'zero_init_rnn': False, 
 'do_zeroshot': False, 
 'oos_threshold': False, 
 'ontology_version': '', 
 'dstlm': False, 
 'vizualization': 0}


args = {
    "model_type": "todbert",         # Specify the model type (e.g., "bert", "gpt")
    "my_model" : "BeliefTracker",
    "usr_token": "[USR]",         # Token to represent user turns
    "sys_token": "[SYS]",         # Token to represent system responses
    "example_type": "turn",       # Specify example type (e.g., "turn" or "dial")
    "task" : "dst",
    "task_name" : "dst",
    "batch_size" : 8, 
    "train_data_ratio": 0.05, 
    "dataset" : "MMConv",
    "rand_seed": 111,
    "max_seq_length" : 256,
    "do_train" : 1,
    "output_dir" : "C:/Users/Zhiyi/Desktop/NLC/project/tod-bert-finetuning/MMConv/finetune/save",
    "nb_runs" : 1,
    "fix_rand_seed" : "store_true",
    "eval_batch_size" : 100,
    "epoch" : 50,
    "eval_by_step":4000,
    "model_name_or_path":"bert-base-uncased",
    "cache_dir": "",
    "n_gpu" : 1
}

# Copy fields from org to args if they do not exist in args
for key, value in org.items():
    if key not in args:
        args[key] = value

print(args)
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Found 2 idle device(s): [1, 3]\n",
      "Please choose which one(s) to use (default: 'all'): 3\n",
      "Using devices: [3]\n",
      "Batch size for evaluation: 8\n",
      "STARTING EVALUATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 870/870 [57:54<00:00,  3.99s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving <class 'collections.defaultdict'> object to all_prediction_mmdial_checkpoint-52560.json...\n",
      "Saved <class 'collections.defaultdict'> object to all_prediction_mmdial_checkpoint-52560.json\n",
      "Loading all_prediction_mmdial_checkpoint-52560.json...\n",
      "Loaded all_prediction_mmdial_checkpoint-52560.json to <class 'dict'> object\n",
      "Belief acc: 0.06394596924845523\n",
      "Action acc: 0.2207213680126455\n",
      "Inform acc: 0.8195142980313264\n",
      "Request acc: 0.7255352780571921\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from utils.model_utils import get_device_ids\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from dataloader_simpletod import MMDialDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.model_utils import CustomPaddingTensorCollator\n",
    "from torch.nn import DataParallel\n",
    "import sys\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from utils.json_utils import save, load\n",
    "import re\n",
    "\n",
    "informable_slots = {'wheelchair accessible', 'reservations', 'restroom',\n",
    "                    'smoking', 'credit cards', 'outdoor seating', 'parking',\n",
    "                    'music', 'wi-fi', 'dining options', 'drinks', 'venuescore',\n",
    "                    'menus', 'price', 'venueneigh'}\n",
    "requestable_slots = {'venuename', 'telephone', 'venueaddress'}\n",
    "\n",
    "token_matcher = re.compile(r'<\\|[a-zA-Z]+\\|>')\n",
    "\n",
    "\n",
    "def next_token(text):\n",
    "    result = token_matcher.search(text)\n",
    "    return result if result is None else result[0]\n",
    "\n",
    "\n",
    "def get_token_text(token):\n",
    "    return token.replace('<', '').replace('>', '').replace('|', '').replace('[', '').replace(']', '')\n",
    "\n",
    "\n",
    "def extract(text, begin_token, end_token=None, no_token_in_between=True):\n",
    "    end_token = end_token or f'<|endof{get_token_text(begin_token)}|>'\n",
    "    begin_idx = text.find(begin_token)\n",
    "    if begin_idx == -1:\n",
    "        return '', None\n",
    "    begin_with_len = begin_idx + len(begin_token)\n",
    "    end_idx = text[begin_with_len:].find(end_token)\n",
    "    if end_idx == -1:\n",
    "        return '', None\n",
    "    end_idx += begin_with_len\n",
    "    next_token_ = next_token(text[begin_with_len:])\n",
    "    if not no_token_in_between or next_token_ == end_token:\n",
    "        return text[begin_with_len: end_idx].strip(), begin_idx\n",
    "    recurse_result = extract(text[begin_with_len:], begin_token, end_token=end_token, no_token_in_between=no_token_in_between)\n",
    "    return recurse_result[0], (recurse_result[1] + begin_with_len) if recurse_result[1] is not None else None\n",
    "\n",
    "\n",
    "transformer_model = \"gpt2\"\n",
    "\n",
    "device_ids = get_device_ids(cuda=True)\n",
    "print('Using devices: {}'.format(device_ids))\n",
    "\n",
    "BATCH_SIZE_EVAL = 8\n",
    "print('Batch size for evaluation: {}'.format(BATCH_SIZE_EVAL))\n",
    "\n",
    "workers_eval = max(min(8, BATCH_SIZE_EVAL >> 3), 4)\n",
    "\n",
    "\n",
    "paths = ['./resources/test.simpletod']\n",
    "test = MMDialDataset.create_data(paths, transformer_model, split=(1,), shuffle=False)\n",
    "\n",
    "first = {\n",
    "    'context_input_ids': True,\n",
    "    'context_attention_mask': True\n",
    "}\n",
    "key2pad_id = {\n",
    "    'context_input_ids': test.tokenizer.eos_token_id,\n",
    "    'labels': test.tokenizer.eos_token_id\n",
    "}\n",
    "collator = CustomPaddingTensorCollator(first=first, key2pad_id=key2pad_id)\n",
    "dataloader_test = DataLoader(test, batch_size=BATCH_SIZE_EVAL, shuffle=False, num_workers=workers_eval, pin_memory=True, drop_last=False, collate_fn=collator)\n",
    "\n",
    "\n",
    "# **here to choose the checkpoint **\n",
    "checkpoint = './output/gpt2/checkpoint-XXXX'\n",
    "config = GPT2Config.from_pretrained(os.path.join(checkpoint, 'config.json'))\n",
    "model = GPT2LMHeadModel.from_pretrained(os.path.join(checkpoint, 'pytorch_model.bin'), config=config)\n",
    "model.eval()\n",
    "model_device = device_ids[0]\n",
    "if len(device_ids) > 1:\n",
    "    model = DataParallel(model, device_ids=device_ids, output_device=device_ids[-1])\n",
    "model.to(f'cuda:{model_device}')\n",
    "\n",
    "\n",
    "def make_position_ids(attention_mask):\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 0)\n",
    "    return position_ids\n",
    "\n",
    "\n",
    "def slot_in_slots(slot, slots):\n",
    "    if not slot.strip():\n",
    "        return False\n",
    "    slot_split = slot.split()\n",
    "    return slot_split[0] in slots or ' '.join(slot_split[:2]) in slots\n",
    "\n",
    "\n",
    "def get_belief(belief, slots=None):\n",
    "    return [x for x in belief.split(', ') if slots is None or slot_in_slots(x, slots)]\n",
    "\n",
    "\n",
    "def shift_past(past, shift=1):\n",
    "    return tuple(p[:, :, :, shift:, :] for p in past)\n",
    "\n",
    "\n",
    "max_len = 800\n",
    "with torch.no_grad():\n",
    "    print(\"STARTING EVALUATION\")\n",
    "    all_predictions = defaultdict(list)\n",
    "    pbar = tqdm(enumerate(dataloader_test), total=len(dataloader_test))\n",
    "    end_tokens = [1279, 91, 437, 1659, 26209, 91, 29] # <|endofresponse|>\n",
    "    end_tokens = torch.tensor(end_tokens).to(model_device).view(-1)\n",
    "    for i, batch in pbar:\n",
    "        input_ids = batch['context_input_ids'].long().to(model_device)\n",
    "        attention_mask = batch['context_attention_mask'].long().to(model_device)\n",
    "        labels = batch['labels'].long()\n",
    "        labels_len = batch['labels_len'].long()\n",
    "        ids = batch['id'].long()\n",
    "\n",
    "        input_ids = input_ids.to(model_device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(model_device)\n",
    "\n",
    "        generation_mask = torch.full((input_ids.shape[0], end_tokens.shape[0]), -1, dtype=int).to(model_device)\n",
    "        next_tokens = input_ids\n",
    "        predictions = []\n",
    "        past = None\n",
    "        for i in range(max_len):\n",
    "            position_ids = make_position_ids(attention_mask)\n",
    "            if past is not None:\n",
    "                position_ids = position_ids[:, -1]\n",
    "            outputs = model(next_tokens, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past, use_cache=True, return_dict=True)\n",
    "            logits = outputs['logits'][:, -1].unsqueeze(1)\n",
    "            past = outputs['past_key_values']\n",
    "            next_tokens = logits.argmax(dim=-1)\n",
    "            if i < generation_mask.shape[1]:\n",
    "                diff_mask = torch.tensor(1).to(model_device)\n",
    "                generation_mask[:, i] = next_tokens.view(-1)\n",
    "            else:\n",
    "                diff_mask = (generation_mask != end_tokens).any(dim=1)\n",
    "                if generation_mask[diff_mask].nelement() == 0:\n",
    "                    break\n",
    "                generation_mask[diff_mask] = generation_mask[diff_mask].roll(-1, 1)\n",
    "                generation_mask[diff_mask, -1] = next_tokens.view(-1)[diff_mask]\n",
    "            if attention_mask.shape[1] < 1024:\n",
    "                attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), dtype=int).to(model_device) * diff_mask.view(-1, 1)], dim=1)\n",
    "            else:\n",
    "                attention_mask = attention_mask.roll(-1, 1)\n",
    "                attention_mask[:, -1] = torch.ones(attention_mask.shape[0], dtype=int).to(model_device) * diff_mask.view(-1)\n",
    "                past = shift_past(past)\n",
    "            predicted_tokens = next_tokens.detach()\n",
    "            predicted_tokens[attention_mask[:, -1] == 0] = test.tokenizer.eos_token_id\n",
    "            predictions.append(predicted_tokens)\n",
    "        predictions = torch.cat(predictions, dim=1)\n",
    "\n",
    "        for j, gt in enumerate(labels):\n",
    "            gt = gt[:labels_len[j]].tolist()\n",
    "            pred = predictions[j][predictions[j] != test.tokenizer.eos_token_id].tolist()\n",
    "            gt_text = test.tokenizer.decode(gt)\n",
    "            pred_text = test.tokenizer.decode(pred)\n",
    "            all_predictions[ids[j].item()].append({\n",
    "                'response_prediction': MMDialDataset.extract(pred_text, '<|response|>', keep_tokens=True),\n",
    "                'response_gt': MMDialDataset.extract(gt_text, '<|response|>', keep_tokens=True),\n",
    "                'belief_prediction': extract(pred_text, '<|belief|>')[0],\n",
    "                'belief_gt': extract(gt_text, '<|belief|>')[0],\n",
    "                'action_prediction': extract(pred_text, '<|action|>')[0],\n",
    "                'action_gt': extract(gt_text, '<|action|>')[0]\n",
    "            })\n",
    "\n",
    "    cpn = checkpoint[checkpoint.rindex('/') + 1:]\n",
    "    save(all_predictions, f'all_prediction_mmdial_{cpn}.json')\n",
    "\n",
    "    cpn = checkpoint[checkpoint.rindex('/') + 1:]\n",
    "    all_predictions = load(f'all_prediction_mmdial_{cpn}.json')\n",
    "\n",
    "    score_belief = 0\n",
    "    score_action = 0\n",
    "    score_inform = 0\n",
    "    score_request = 0\n",
    "    total = 0\n",
    "    for predictions in all_predictions.values():\n",
    "        for prediction in predictions:\n",
    "            total += 1\n",
    "            belief_prediction = set(get_belief(prediction['belief_prediction']))\n",
    "            belief_gt = set(get_belief(prediction['belief_gt']))\n",
    "            belief_correct = belief_prediction == belief_gt\n",
    "            inform_prediction = set(get_belief(prediction['action_prediction'], informable_slots))\n",
    "            inform_gt = set(get_belief(prediction['action_gt'], informable_slots))\n",
    "            inform_correct = inform_prediction == inform_gt\n",
    "            request_prediction = set(get_belief(prediction['action_prediction'], requestable_slots))\n",
    "            request_gt = set(get_belief(prediction['action_gt'], requestable_slots))\n",
    "            request_correct = request_prediction == request_gt\n",
    "            if belief_correct:\n",
    "                score_belief += 1\n",
    "            if inform_correct:\n",
    "                score_inform += 1\n",
    "            if request_correct:\n",
    "                score_request += 1\n",
    "            action_prediction = set(get_belief(prediction['action_prediction']))\n",
    "            action_gt = set(get_belief(prediction['action_gt']))\n",
    "            action_correct = action_prediction == action_gt\n",
    "            if action_correct:\n",
    "                score_action += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6959 6959\n"
     ]
    }
   ],
   "source": [
    "hyp, ref = [], []\n",
    "for predictions in all_predictions.values():\n",
    "        for prediction in predictions:\n",
    "            hyp.append(prediction[\"response_prediction\"])\n",
    "            ref.append(prediction[\"response_gt\"])\n",
    "print(len(hyp), len(ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "class BLEUScorer(object):\n",
    "    ## BLEU score calculator via GentScorer interface\n",
    "    ## it calculates the BLEU-4 by taking the entire corpus in\n",
    "    ## Calulate based multiple candidates against multiple references\n",
    "    def score(self, hypothesis, corpus, n=1, bleu_level=4):\n",
    "        # containers\n",
    "        count = [0, 0, 0, 0]\n",
    "        clip_count = [0, 0, 0, 0]\n",
    "        r = 0\n",
    "        c = 0\n",
    "        weights = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "        # hypothesis = [hypothesis]\n",
    "        # corpus = [corpus]\n",
    "        # ipdb.set_trace()\n",
    "\n",
    "        # accumulate ngram statistics\n",
    "        for hyps, refs in zip(hypothesis, corpus):\n",
    "            hyps = [hyp.split() for hyp in hyps]\n",
    "            refs = [ref.split() for ref in refs]\n",
    "            # hyps = [hyps]\n",
    "            # hyps = hyps\n",
    "            # Shawn's evaluation\n",
    "            # refs[0] = [u'GO_'] + refs[0] + [u'EOS_']\n",
    "            # hyps[0] = [u'GO_'] + hyps[0] + [u'EOS_']\n",
    "            # ipdb.set_trace()\n",
    "            for idx, hyp in enumerate(hyps):\n",
    "                for i in range(bleu_level):\n",
    "                    # accumulate ngram counts\n",
    "                    hypcnts = Counter(ngrams(hyp, i + 1))\n",
    "                    cnt = sum(hypcnts.values())\n",
    "                    count[i] += cnt\n",
    "\n",
    "                    # compute clipped counts\n",
    "                    max_counts = {}\n",
    "                    for ref in refs:\n",
    "                        refcnts = Counter(ngrams(ref, i + 1))\n",
    "                        for ng in hypcnts:\n",
    "                            max_counts[ng] = max(max_counts.get(ng, 0), refcnts[ng])\n",
    "                    clipcnt = dict((ng, min(count, max_counts[ng])) \\\n",
    "                                   for ng, count in hypcnts.items())\n",
    "                    clip_count[i] += sum(clipcnt.values())\n",
    "\n",
    "                # accumulate r & c\n",
    "                bestmatch = [1000, 1000]\n",
    "                for ref in refs:\n",
    "                    if bestmatch[0] == 0: break\n",
    "                    diff = abs(len(ref) - len(hyp))\n",
    "                    if diff < bestmatch[0]:\n",
    "                        bestmatch[0] = diff\n",
    "                        bestmatch[1] = len(ref)\n",
    "                r += bestmatch[1]\n",
    "                c += len(hyp)\n",
    "                if n == 1:\n",
    "                    break\n",
    "        # computing bleu score\n",
    "        p0 = 1e-7\n",
    "        bp = 1 if c > r else math.exp(1 - float(r) / float(c))\n",
    "        p_ns = [float(clip_count[i]) / float(count[i] + p0) + p0 \\\n",
    "                for i in range(bleu_level)]\n",
    "        s = math.fsum(w * math.log(p_n) \\\n",
    "                      for w, p_n in zip(weights, p_ns) if p_n)\n",
    "        bleu = bp * math.exp(s)\n",
    "        return bleu\n",
    "\n",
    "\n",
    "def remove_punctuation(text, keep=False):\n",
    "    if keep:\n",
    "        replace_pattern = ' \\g<punc> ' # Insert spaces before and after punctuations\n",
    "    else:\n",
    "        replace_pattern = ' ' # Remove punctuations\n",
    "    text = re.sub(r'(?P<punc>[^a-zA-Z\\d \\[\\]\\|\\<\\>]+)', replace_pattern, text)\n",
    "    text = re.sub(' {2,}', ' ', text)\n",
    "    # print(text)\n",
    "    # input()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "bleu_evaluator = BLEUScorer()\n",
    "new_hyp = [[remove_punctuation(line.replace('<|response|>', '').replace('<|endofresponse|>', '').replace('<|system|>', '')).strip()] for line in hyp]\n",
    "new_ref = [[remove_punctuation(line.replace('<|response|>', '').replace('<|endofresponse|>', '').replace('<|system|>', '')).strip()] for line in ref]\n",
    "print(bleu_evaluator.score(new_hyp, new_ref, n=9999, bleu_level=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
